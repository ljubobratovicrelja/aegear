{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gdown\n",
    "\n",
    "from aegear.model import EfficientUNet, SiameseTracker\n",
    "from aegear.datasets import TrackingDataset, RandomPoissonNoise\n",
    "from aegear.utils import get_latest_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ffc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../data/training'\n",
    "video_dir = \"../data/video\"\n",
    "\n",
    "annotations = {\n",
    "    \"E7\": {\n",
    "        \"file\": 'tracking_E7_clean.json',\n",
    "        \"annotation_gdrive_id\": \"1b8UgWED20Gtkn0ovqizONTlFH-oNYJ_a\",\n",
    "        \"video_gdrive_id\": \"1FJypI1WmZJj4iEo9nVfBvP8FjaoDf1Ob\"\n",
    "    },\n",
    "    \"K9\": {\n",
    "        \"file\": 'tracking_K9_clean.json',\n",
    "        \"annotation_gdrive_id\": \"12jLIHBySVG3G0Ie52NlmRHQ7U3Sv6ca-\",\n",
    "        \"video_gdrive_id\": \"1iZMfmCJnYsIxBIXqVwRrpubO2-_uItHU\"\n",
    "    },\n",
    "    \"S1\": {\n",
    "        \"file\": 'tracking_S1_clean.json',\n",
    "        \"annotation_gdrive_id\": \"1VKNPRTnNb0n5oUBG9hx3ZkLns9CMcs7R\",\n",
    "        \"video_gdrive_id\": \"1aGFLRtPjjiUZjY8k5xH-EKF3sHQSfNnE\"\n",
    "    },\n",
    "    \"4_per_23\": {\n",
    "        \"file\": \"tracking_4_per_23_clean.json\",\n",
    "        \"annotation_gdrive_id\": \"19EHp0idpYDbjqcR-LDWJhYmDBNmkL2ZY\",\n",
    "        \"video_gdrive_id\": \"1A_bdzGDHy3JZo4x7gte_Uv09zQwDbLby\"\n",
    "    },\n",
    "    \"5_per_12\": {\n",
    "        \"file\": \"tracking_5_per_12_clean.json\",\n",
    "        \"annotation_gdrive_id\": \"1GTzxjlaBkqrJfBUoT1dq0TMaR21OXsva\",\n",
    "        \"video_gdrive_id\": \"1Mx5Xsuat4nx_xvttZi04qpaKeywRSEhJ\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create directories if they do not exist.\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "bar = tqdm(annotations.items(), desc=\"Downloading dataset and videos\")\n",
    "for key, annotation_settings in bar:\n",
    "    bar.set_postfix_str(key)\n",
    "\n",
    "    annotations_file = os.path.join(dataset_dir, annotation_settings[\"file\"])\n",
    "    video_file = os.path.join(video_dir, f\"{key}.MOV\")\n",
    "    annotation_file_id = annotation_settings[\"annotation_gdrive_id\"]\n",
    "    video_file_id = annotation_settings[\"video_gdrive_id\"]\n",
    "\n",
    "    # Check if each of the dataset units is present\n",
    "    if not os.path.exists(annotations_file):\n",
    "        print(\"Downloading dataset...\")\n",
    "        url = f'https://drive.google.com/uc?id={annotation_file_id}'\n",
    "        gdown.download(url, annotations_file, quiet=False)\n",
    "\n",
    "    if not os.path.exists(video_file):\n",
    "        print(\"Downloading video...\")\n",
    "        url = f'https://drive.google.com/uc?id={video_file_id}'\n",
    "\n",
    "        gdown.download(url, video_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence(heatmap):\n",
    "    b, _, _, w = heatmap.shape\n",
    "    flat_idx = torch.argmax(heatmap.view(b, -1), dim=1)\n",
    "    y = flat_idx // w\n",
    "    x = flat_idx % w\n",
    "    return heatmap[0, 0, y, x].item()\n",
    "\n",
    "def overlay_heatmap_on_rgb(rgb_tensor, heatmap, alpha=0.5, centroid_color=(0, 1, 0)):\n",
    "    \"\"\"\n",
    "    Overlay heatmap onto RGB image and draw a circle at the predicted centroid.\n",
    "    \n",
    "    Args:\n",
    "        rgb_tensor: [3, H, W] tensor\n",
    "        heatmap: [H, W] numpy array\n",
    "        alpha: blending weight\n",
    "        centroid_color: (R, G, B) tuple in range 0â€“1\n",
    "    Returns:\n",
    "        overlay: [H, W, 3] numpy image\n",
    "    \"\"\"\n",
    "    rgb = rgb_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    rgb = rgb * 0.229 + 0.485\n",
    "    rgb = rgb.clip(0, 1)\n",
    "\n",
    "    heatmap_color = plt.cm.hot(heatmap)[..., :3]\n",
    "    overlay = (1 - alpha) * rgb + alpha * heatmap_color\n",
    "\n",
    "    # Find centroid\n",
    "    flat_idx = heatmap.reshape(-1).argmax()\n",
    "    h, w = heatmap.shape\n",
    "    cy = flat_idx // w\n",
    "    cx = flat_idx % w\n",
    "\n",
    "    # Draw circle\n",
    "    overlay_uint8 = (overlay * 255).astype(np.uint8)\n",
    "    cx_int, cy_int = int(cx), int(cy)\n",
    "    color_bgr = tuple(int(c * 255) for c in reversed(centroid_color))\n",
    "    cv2.circle(overlay_uint8, (cx_int, cy_int), 4, color_bgr, thickness=1)\n",
    "\n",
    "    return overlay_uint8 / 255.0\n",
    "\n",
    "def save_epoch_visualization(val_results, stage, epoch, output_dir=\"vis_epochs\", N=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Sort by distance\n",
    "    sorted_results = sorted(val_results, key=lambda r: r['confidence'], reverse=True)\n",
    "    worst = sorted_results[-N:]\n",
    "    best = sorted_results[:N]\n",
    "    middle = sorted_results[len(sorted_results)//2 - N//2 : len(sorted_results)//2 + N//2]\n",
    "\n",
    "    samples = worst[::-1] + middle + best  # worst at top, best at bottom\n",
    "    total_rows = len(samples)\n",
    "\n",
    "    _, axes = plt.subplots(total_rows, 3, figsize=(9, 3 * total_rows))\n",
    "\n",
    "    for i, result in enumerate(samples):\n",
    "        template = result['template']\n",
    "        search = result['search']\n",
    "        gt = result['gt_heatmap']\n",
    "        pred = result['pred_heatmap']\n",
    "        xg, yg = result['gt_centroid']\n",
    "        xp, yp = result['pred_centroid']\n",
    "        confidence = result['confidence']\n",
    "\n",
    "        template_img = TF.to_pil_image(denormalize(template))\n",
    "        search_img = TF.to_pil_image(denormalize(search))\n",
    "\n",
    "        search_np = TF.to_tensor(search_img).permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Normalize heatmaps\n",
    "        pred_norm = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "        gt_norm = (gt - gt.min()) / (gt.max() - gt.min() + 1e-8)\n",
    "        diff_norm = np.abs(pred_norm - gt_norm)\n",
    "\n",
    "        # Overlay prediction heatmap\n",
    "        overlay = 0.6 * search_np + 0.4 * plt.cm.jet(pred_norm)[..., :3]\n",
    "        overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "        # Diff colormap\n",
    "        diff_rgb = plt.cm.magma(diff_norm)[..., :3]\n",
    "\n",
    "        # Plot\n",
    "        axes[i, 0].imshow(template_img)\n",
    "        axes[i, 0].set_title(f\"Template idx {i}\")\n",
    "\n",
    "        axes[i, 1].imshow(overlay)\n",
    "        axes[i, 1].scatter([xp], [yp], c='red', marker='x', label='Pred')\n",
    "        axes[i, 1].scatter([xg], [yg], c='green', marker='o', label='GT')\n",
    "        axes[i, 1].set_title(f\"Search | Conf: {confidence:.2f}\")\n",
    "        axes[i, 1].legend()\n",
    "\n",
    "        axes[i, 2].imshow(diff_rgb)\n",
    "        axes[i, 2].set_title(\"Abs Diff\")\n",
    "\n",
    "        for ax in axes[i]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(output_dir, f\"epoch_stage_{stage:03d}_{epoch:03d}.png\")\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "def save_epoch_activation_visualization(val_results, model, device, stage_n, epoch,\n",
    "                                        output_dir=\"vis_activations\", N=5, channels_per_stage=3):\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def get_activations(model):\n",
    "        activations = {}\n",
    "\n",
    "        def get_activation(name):\n",
    "            def hook(_, __, output):\n",
    "                activations[name] = output.detach().cpu()\n",
    "            return hook\n",
    "\n",
    "        model.enc3.register_forward_hook(get_activation('enc3'))\n",
    "        model.enc4.register_forward_hook(get_activation('enc4'))\n",
    "        model.enc4.register_forward_hook(get_activation('enc5'))\n",
    "        model.up4.register_forward_hook(get_activation('up4'))\n",
    "        model.up3.register_forward_hook(get_activation('up3'))\n",
    "        model.up2.register_forward_hook(get_activation('up2'))\n",
    "        model.up1.register_forward_hook(get_activation('up1'))\n",
    "        model.up0.register_forward_hook(get_activation('up0'))\n",
    "        model.out.register_forward_hook(get_activation('out'))\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def denormalize(img_tensor):\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(img_tensor.device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(img_tensor.device)\n",
    "        return (img_tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "    activations = get_activations(model)\n",
    "\n",
    "    # Sort and group samples\n",
    "    sorted_results = sorted(val_results, key=lambda r: r['confidence'], reverse=True)\n",
    "    worst = sorted_results[-N:][::-1]\n",
    "    best = sorted_results[:N]\n",
    "    middle = sorted_results[len(sorted_results)//2 - N//2 : len(sorted_results)//2 + N//2]\n",
    "    samples = worst + middle + best\n",
    "    total_rows = len(samples)\n",
    "\n",
    "    stages = ['enc3', 'enc4', 'enc5', 'up4', 'up3', 'up2', 'up1', 'up0', 'out']\n",
    "    n_cols = 1 + channels_per_stage * len(stages)\n",
    "\n",
    "    fig, axs = plt.subplots(total_rows, n_cols, figsize=(n_cols * 2.5, total_rows * 3))\n",
    "    if total_rows == 1:\n",
    "        axs = axs[None, :]  # force 2D\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for row, sample in enumerate(samples):\n",
    "        template = sample['template'].unsqueeze(0).to(device)\n",
    "        search = sample['search'].unsqueeze(0).to(device)\n",
    "\n",
    "        xg, yg = sample['gt_centroid']\n",
    "        xp, yp = sample['pred_centroid']\n",
    "        confidence = sample['confidence']\n",
    "        heatmap = sample['pred_heatmap']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model(template, search)\n",
    "\n",
    "        # Overlay heatmap on search image\n",
    "        img_np = denormalize(search[0]).permute(1, 2, 0).cpu().numpy()\n",
    "        hm_np = heatmap.numpy()\n",
    "\n",
    "        overlay = img_np.copy()\n",
    "        overlay[..., 0] = np.clip(overlay[..., 0] + 0.5 * hm_np, 0, 1)\n",
    "\n",
    "        axs[row, 0].imshow(overlay)\n",
    "        axs[row, 0].scatter([xg], [yg], c='green', marker='o', label='GT')\n",
    "        axs[row, 0].scatter([xp], [yp], c='red', marker='x', label='Pred')\n",
    "        axs[row, 0].set_title(f'Conf: {confidence:.2f}')\n",
    "        axs[row, 0].axis('off')\n",
    "        axs[row, 0].legend()\n",
    "\n",
    "        # Activations\n",
    "        col = 1\n",
    "        for stage in stages:\n",
    "            act = activations[stage][0]\n",
    "            for ch in range(channels_per_stage):\n",
    "                if ch < act.shape[0]:\n",
    "                    axs[row, col].imshow(act[ch].numpy(), cmap='viridis')\n",
    "                    axs[row, col].set_title(f'{stage} | Ch {ch}')\n",
    "                else:\n",
    "                    axs[row, col].axis('off')\n",
    "                axs[row, col].axis('off')\n",
    "                col += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(output_dir, f\"activation_stage_{stage_n:03d}_epoch_{epoch:03d}.png\")\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def denormalize(t):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "    return t * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_frame_seek = list(range(1, 15))\n",
    "interpolation_smoothness = 5.0\n",
    "\n",
    "dataset_annotations = [os.path.join(dataset_dir, annotation_settings[\"file\"]) for annotation_settings in annotations.values()]\n",
    "\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    RandomPoissonNoise(p=0.15),\n",
    "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.05, 0.75)),\n",
    "    transforms.ColorJitter(brightness=0.35, contrast=0.25, saturation=0.25, hue=0.15),\n",
    "])\n",
    "\n",
    "train_fraction = 0.9\n",
    "train_dataset, val_dataset = TrackingDataset.build_split_datasets(\n",
    "    dataset_annotations,\n",
    "    video_dir,\n",
    "    train_fraction=train_fraction,\n",
    "    future_frame_seek=future_frame_seek,\n",
    "    interpolation_smoothness=interpolation_smoothness,\n",
    "    augmentation_transforms=augmentation_transforms,\n",
    "    gaussian_sigma=12.0,\n",
    "    rotation_range=5.0,\n",
    "    scale_range=0.25)\n",
    "\n",
    "# Subsample the validation set\n",
    "indices = list(range(len(val_dataset)))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "val_indices = indices[:int(len(train_dataset) * (1.0 - train_fraction))]\n",
    "val_dataset = torch.utils.data.Subset(val_dataset, val_indices)\n",
    "\n",
    "# Setup the batch size and epoch limiting for training.\n",
    "batch_size = 128\n",
    "number_of_samples_per_epoch = 3000\n",
    "\n",
    "sampler = torch.utils.data.RandomSampler(train_dataset, replacement=True, num_samples=number_of_samples_per_epoch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac116be",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10  # Number of samples to visualize\n",
    "\n",
    "fig, axes = plt.subplots(N, 3, figsize=(8, 2 * N))\n",
    "\n",
    "for i in range(N):\n",
    "    random.seed(i)\n",
    "    idx = random.randint(0, len(train_dataset) - 1)\n",
    "    template, search, heatmap = train_dataset[idx]\n",
    "\n",
    "    template_img = TF.to_pil_image(denormalize(template))\n",
    "    search_img = TF.to_pil_image(denormalize(search))\n",
    "    heatmap_np = heatmap.squeeze().numpy()\n",
    "\n",
    "    # === Resize heatmap\n",
    "    search_w, search_h = search_img.size  # PIL: size = (W, H)\n",
    "    heatmap_np = cv2.resize(heatmap_np, (search_w, search_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # === Normalize heatmap for display\n",
    "    heatmap_norm = (heatmap_np - heatmap_np.min()) / (heatmap_np.max() - heatmap_np.min() + 1e-8)\n",
    "\n",
    "    # === Blend\n",
    "    search_np = TF.to_tensor(search_img).permute(1, 2, 0).numpy()\n",
    "    heatmap_rgb = plt.cm.jet(heatmap_norm)[..., :3]\n",
    "    overlay = 0.6 * search_np + 0.4 * heatmap_rgb\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    # === Plot\n",
    "    axes[i, 0].imshow(template_img)\n",
    "    axes[i, 0].set_title(\"Template\")\n",
    "\n",
    "    axes[i, 1].imshow(search_img)\n",
    "    axes[i, 1].set_title(\"Search\")\n",
    "\n",
    "    axes[i, 2].imshow(overlay)\n",
    "    axes[i, 2].set_title(\"Overlay\")\n",
    "\n",
    "    for ax in axes[i]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e80f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root model path\n",
    "pretrained_model_dir = '../models/'\n",
    "\n",
    "assert os.path.exists(pretrained_model_dir), f\"Pretrained model directory {pretrained_model_dir} does not exist.\"\n",
    "\n",
    "model_dir = '../data/training/models/siamese'\n",
    "log_dir = f'{model_dir}/runs'\n",
    "checkpoint_dir = f'{model_dir}/checkpoints'\n",
    "epoch_vis = f'{model_dir}/epoch_vis'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(epoch_vis, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8abe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device definition.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Model loaded and moved to device:\", device)\n",
    "\n",
    "def str2bool(val):\n",
    "    return str(val).lower() in (\"1\", \"true\", \"yes\", \"on\")\n",
    "\n",
    "# Model settings for training\n",
    "continue_training = str2bool(os.environ.get(\"CONTINUE_TRAINING\", \"false\"))  # Continue training from the latest production model\n",
    "use_best_model = str2bool(os.environ.get(\"USE_BEST_MODEL\", \"false\"))  # If contnue_training is True, uses the cached best_model.pth, else uses the latest production model.\n",
    "\n",
    "if continue_training:\n",
    "   if use_best_model:\n",
    "      # Load the best model\n",
    "      best_model_path = os.path.join(model_dir, \"best_model.pth\")\n",
    "      assert os.path.exists(best_model_path)\n",
    "\n",
    "   else:     \n",
    "      siamese_model_filename = \"model_siamese\"\n",
    "      best_model_path = get_latest_model_path(pretrained_model_dir, siamese_model_filename)\n",
    "\n",
    "   print(\"Continuing training of the Siamese model from:\", best_model_path)\n",
    "\n",
    "   model = SiameseTracker()\n",
    "   model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "else:\n",
    "   efficient_unet_filename = \"model_efficient_unet\"\n",
    "   latest_model_stats = get_latest_model_path(pretrained_model_dir, efficient_unet_filename)\n",
    "\n",
    "   unet = EfficientUNet()\n",
    "   unet.load_state_dict(torch.load(latest_model_stats, map_location=device))\n",
    "   unet.eval()\n",
    "\n",
    "   model = SiameseTracker(unet)\n",
    "\n",
    "   print(\"Training a new Siamese model with EfficientNet encoder from:\", latest_model_stats)\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f86280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_bce_with_logits(pred, target, limit=0.5, pos_weight=10.0):\n",
    "    weights = torch.ones_like(target)\n",
    "    weights[target > limit] = pos_weight  # emphasize center of Gaussian\n",
    "\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target, weight=weights, reduction='mean')\n",
    "    return bce\n",
    "\n",
    "def get_centroids_per_sample(heatmap):\n",
    "    b, _, _, w = heatmap.shape\n",
    "    heatmaps = heatmap.squeeze(1)  # shape: [B, H, W]\n",
    "    centroids = []\n",
    "\n",
    "    for i in range(b):\n",
    "        hm = heatmaps[i]\n",
    "        hm_sum = hm.mean().item()\n",
    "\n",
    "        if hm_sum < 1e-8:\n",
    "            centroids.append(None)\n",
    "        else:\n",
    "            flat_idx = torch.argmax(hm)\n",
    "            y = flat_idx // w\n",
    "            x = flat_idx % w\n",
    "            conf = hm[y, x]\n",
    "            centroids.append((x.float(), y.float(), conf.float()))\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def centroid_distance_loss(pred, target):\n",
    "    preds = get_centroids_per_sample(torch.sigmoid(pred))\n",
    "    targets = get_centroids_per_sample(target)\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for p, t in zip(preds, targets):\n",
    "        if p is not None and t is not None:\n",
    "            x_p, y_p, _ = p\n",
    "            x_t, y_t, _ = t\n",
    "            dist = torch.sqrt((x_p - x_t) ** 2 + (y_p - y_t) ** 2 + 1e-8)\n",
    "            distances.append(dist)\n",
    "\n",
    "    if not distances:\n",
    "        return torch.tensor(0.0).to(pred.device), 0\n",
    "\n",
    "    return torch.stack(distances).mean(), len(distances)\n",
    "\n",
    "def rgb_consistency_loss(template_img, search_img, pred_heatmap, sigma=2.0, threshold=0.5):\n",
    "    B, _, H, W = template_img.shape\n",
    "    device = template_img.device\n",
    "\n",
    "    # === Upsample predicted heatmap\n",
    "    pred_heatmap_up = F.interpolate(pred_heatmap, size=(H, W), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # === Create fixed centered Gaussian for all batch\n",
    "    grid_y, grid_x = torch.meshgrid(\n",
    "        torch.linspace(0, H - 1, H, device=device),\n",
    "        torch.linspace(0, W - 1, W, device=device),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    center_y = (H - 1) / 2\n",
    "    center_x = (W - 1) / 2\n",
    "    gaussian = torch.exp(-((grid_x - center_x)**2 + (grid_y - center_y)**2) / (2 * sigma**2))\n",
    "    gaussian /= gaussian.sum() + 1e-8\n",
    "    gaussian = gaussian[None, None, :, :]  # shape (1, 1, H, W)\n",
    "\n",
    "    loss = 0.0\n",
    "    for i in range(B):\n",
    "        # === Mask and normalize predicted heatmap\n",
    "        mask = (pred_heatmap_up[i] > threshold).float()\n",
    "        weighted_mask = pred_heatmap_up[i] * mask\n",
    "        weighted_mask /= weighted_mask.sum() + 1e-8  # (1, H, W)\n",
    "\n",
    "        # === Compute mean RGB in search\n",
    "        rgb_search = (search_img[i] * weighted_mask).view(3, -1).sum(dim=1)\n",
    "\n",
    "        # === Compute mean RGB in template using Gaussian\n",
    "        rgb_template = (template_img[i] * gaussian[0]).view(3, -1).sum(dim=1)\n",
    "\n",
    "        loss += F.mse_loss(rgb_search, rgb_template)\n",
    "\n",
    "    return loss / B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973bbf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stages = [\n",
    "   {\n",
    "       \"freeze_layers\": [\n",
    "            model.enc1,\n",
    "            model.enc2,\n",
    "            model.enc3,\n",
    "            model.enc4,\n",
    "            model.enc5,\n",
    "       ],\n",
    "       \"epochs\": 5,\n",
    "       \"lr\": 1e-5,\n",
    "   },\n",
    "   {\n",
    "       \"freeze_layers\": [\n",
    "            model.enc1,\n",
    "            model.enc2,\n",
    "            model.enc3,\n",
    "            model.enc4,\n",
    "            model.enc5,\n",
    "            model.up4,\n",
    "            model.up3,\n",
    "            model.up2,\n",
    "            model.up1,\n",
    "       ],\n",
    "       \"epochs\": 5,\n",
    "       \"lr\": 1.25e-3,\n",
    "   },\n",
    "   {\n",
    "       \"freeze_layers\": [\n",
    "            model.enc1,\n",
    "            model.enc2,\n",
    "            model.enc3,\n",
    "            model.enc4,\n",
    "            model.enc5,\n",
    "            model.up4,\n",
    "            model.up3,\n",
    "            model.up2,\n",
    "       ],\n",
    "       \"epochs\": 5,\n",
    "       \"lr\": 2.5e-4,\n",
    "   },\n",
    "   {\n",
    "       \"freeze_layers\": [\n",
    "            model.enc1,\n",
    "            model.enc2,\n",
    "            model.enc3,\n",
    "            model.enc4,\n",
    "            model.enc5,\n",
    "            model.up4,\n",
    "            model.up3,\n",
    "       ],\n",
    "       \"epochs\": 5,\n",
    "       \"lr\": 1e-4,\n",
    "   },\n",
    "   {\n",
    "       \"freeze_layers\": [\n",
    "            model.enc1,\n",
    "            model.enc2,\n",
    "            model.enc3,\n",
    "            model.enc4,\n",
    "       ],\n",
    "       \"epochs\": 5,\n",
    "       \"lr\": 1e-4,\n",
    "   },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b90a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "losses = []\n",
    "\n",
    "epoch_save_interval = 1\n",
    "\n",
    "centroid_weight = 2.5e-3  # weight for centroid distance loss\n",
    "color_weight = 5e-3  # weight for color consistency loss\n",
    "sparsity_weight = 1e-3  # weight for sparsity loss\n",
    "\n",
    "\n",
    "for stage, training_stage in enumerate(training_stages):\n",
    "\n",
    "    freeze_layers = training_stage[\"freeze_layers\"]\n",
    "    epochs = training_stage[\"epochs\"]\n",
    "    lr = training_stage[\"lr\"]\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "    )\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for layer in model.children():\n",
    "        if layer not in freeze_layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for layer in freeze_layers:\n",
    "            layer.eval()\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        random.seed(time.time())\n",
    "        train_bar = tqdm(train_loader, desc=f\"Stage {stage + 1}, Training {epoch + 1}\", leave=False)\n",
    "        for template, search, heatmap in train_bar:\n",
    "            template = template.to(device)\n",
    "            search = search.to(device)\n",
    "            target = heatmap.to(device)\n",
    "\n",
    "            output = model(template, search)\n",
    "\n",
    "            mse = weighted_bce_with_logits(output, target)\n",
    "            cdist, distances = centroid_distance_loss(output, target)\n",
    "            rgb_loss = rgb_consistency_loss(template, search, output)\n",
    "            sparsity_loss = output.pow(2).mean()\n",
    "\n",
    "            loss = (\n",
    "                mse\n",
    "                + centroid_weight * cdist\n",
    "                + color_weight * rgb_loss\n",
    "                + sparsity_weight * sparsity_loss\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_results = []  # NEW\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"Stage {stage + 1}, Validation {epoch + 1}\", leave=False)\n",
    "\n",
    "            for template, search, heatmap in val_bar:\n",
    "                template = template.to(device)\n",
    "                search = search.to(device)\n",
    "                target = heatmap.to(device)\n",
    "\n",
    "                output = model(template, search)\n",
    "\n",
    "                mse = weighted_bce_with_logits(output, target)\n",
    "                cdist, distances = centroid_distance_loss(output, target)\n",
    "                rgb_loss = rgb_consistency_loss(template, search, output)\n",
    "                sparsity_loss = output.pow(2).mean()\n",
    "\n",
    "                loss = (\n",
    "                    mse\n",
    "                    + centroid_weight * cdist\n",
    "                    + color_weight * rgb_loss\n",
    "                    + sparsity_weight * sparsity_loss\n",
    "                )\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "                # --------- COLLECT SAMPLE DATA ---------\n",
    "                pred = torch.sigmoid(output)\n",
    "\n",
    "                # Get centroids and confidence per sample\n",
    "                centroids_pred = get_centroids_per_sample(pred)\n",
    "                centroids_gt = get_centroids_per_sample(target)\n",
    "\n",
    "                for i in range(template.size(0)):\n",
    "                    p = centroids_pred[i]\n",
    "                    t = centroids_gt[i]\n",
    "\n",
    "                    if p is None or t is None:\n",
    "                        continue  # skip bad samples\n",
    "\n",
    "                    x_pred, y_pred, confidence = p\n",
    "                    x_gt, y_gt, _ = t\n",
    "\n",
    "                    xp, yp = x_pred.item(), y_pred.item()\n",
    "                    xg, yg = x_gt.item(), y_gt.item()\n",
    "\n",
    "                    dist = np.sqrt((xp - xg) ** 2 + (yp - yg) ** 2)\n",
    "\n",
    "                    val_results.append({\n",
    "                        'template': template[i].cpu(),\n",
    "                        'search': search[i].cpu(),\n",
    "                        'gt_heatmap': target[i, 0].cpu(),\n",
    "                        'pred_heatmap': pred[i, 0].cpu(),\n",
    "                        'gt_centroid': (xg, yg),\n",
    "                        'pred_centroid': (xp, yp),\n",
    "                        'confidence': confidence.item(),\n",
    "                        'distance': dist,\n",
    "                    })\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        losses.append((train_loss, val_loss))\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        save_epoch_visualization(val_results, stage, epoch, output_dir=epoch_vis, N=10)\n",
    "        save_epoch_activation_visualization(val_results, model, device, stage, epoch, output_dir=epoch_vis)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"New best model, saving.\")\n",
    "            torch.save(model.state_dict(), f'{model_dir}/best_model.pth')\n",
    "\n",
    "        if (epoch + 1) % epoch_save_interval == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56eead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses, val_losses = zip(*losses)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41695dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for inference\n",
    "model.load_state_dict(torch.load(f'{model_dir}/best_model.pth'))\n",
    "model.eval()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def574be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "total_distances = []\n",
    "total_confidences = []\n",
    "within_radius = {r: 0 for r in [3, 5, 10]}\n",
    "n_samples = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for templates, searches, heatmaps in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        templates = templates.to(device)\n",
    "        searches = searches.to(device)\n",
    "        heatmaps = heatmaps.to(device)\n",
    "\n",
    "        preds = torch.sigmoid(model(templates, searches))\n",
    "\n",
    "        heatmaps = F.interpolate(heatmaps, size=templates.shape[2:], mode='bilinear', align_corners=False)\n",
    "        preds = F.interpolate(preds, size=templates.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        centroids_pred = get_centroids_per_sample(preds)\n",
    "        centroids_gt = get_centroids_per_sample(heatmaps)\n",
    "\n",
    "        for i in range(len(templates)):\n",
    "            p = centroids_pred[i]\n",
    "            t = centroids_gt[i]\n",
    "\n",
    "            if p is None or t is None:\n",
    "                continue\n",
    "\n",
    "            x_pred, y_pred, confidence = p\n",
    "            x_gt, y_gt, _ = t\n",
    "\n",
    "            xp, yp = x_pred.item(), y_pred.item()\n",
    "            xg, yg = x_gt.item(), y_gt.item()\n",
    "\n",
    "            confidence = confidence.item()\n",
    "\n",
    "            dist = np.sqrt((xp - xg) ** 2 + (yp - yg) ** 2)\n",
    "            total_distances.append(dist)\n",
    "            total_confidences.append(confidence)\n",
    "\n",
    "            for r in within_radius:\n",
    "                if dist <= r:\n",
    "                    within_radius[r] += 1\n",
    "            n_samples += 1\n",
    "\n",
    "            img = searches[i].cpu()\n",
    "            pred_hm = preds[i, 0].cpu()\n",
    "            gt_hm = heatmaps[i, 0].cpu()\n",
    "\n",
    "            results.append({\n",
    "                'image': img,\n",
    "                'gt_heatmap': gt_hm,\n",
    "                'pred_heatmap': pred_hm,\n",
    "                'gt_centroid': (xg, yg),\n",
    "                'pred_centroid': (xp, yp),\n",
    "                'confidence': confidence,\n",
    "                'distance': dist,\n",
    "            })\n",
    "\n",
    "avg_dist = np.mean(total_distances)\n",
    "avg_conf = np.mean(total_confidences)\n",
    "print(f\"\\nAverage centroid distance: {avg_dist:.2f} px, Average confidence: {avg_conf:.2f}\")\n",
    "\n",
    "for r in sorted(within_radius):\n",
    "    print(f\"Within {r}px: {within_radius[r] / n_samples:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort(key=lambda x: -x['distance'])  # descending\n",
    "\n",
    "\n",
    "def show_sample(result, index=None):\n",
    "    img = result['image']\n",
    "    gt = result['gt_heatmap']\n",
    "    pred = result['pred_heatmap']\n",
    "    xg, yg = result['gt_centroid']\n",
    "    xp, yp = result['pred_centroid']\n",
    "    dist = result['distance']\n",
    "    conf = result['confidence']\n",
    "\n",
    "    # Denormalize image for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img_disp = img * std + mean\n",
    "    img_disp = img_disp.clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "\n",
    "    xc, yc = img_disp.shape[1] // 2, img_disp.shape[0] // 2\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "    axs[0].imshow(img_disp)\n",
    "    axs[0].scatter([xg], [yg], c='green', label='GT', marker='o')\n",
    "    axs[0].scatter([xp], [yp], c='red', label='Pred', marker='o')\n",
    "    axs[0].scatter([xc], [yc], c='green', label='Template', marker='x')\n",
    "    \n",
    "    axs[0].set_title(f'Image (Err: {dist:.1f}px, Conf: {conf:.2f})')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].imshow(gt.numpy(), cmap='hot')\n",
    "    axs[1].set_title('GT Heatmap')\n",
    "\n",
    "    axs[2].imshow(pred.numpy(), cmap='hot')\n",
    "    axs[2].set_title('Predicted Heatmap')\n",
    "\n",
    "    if index is not None:\n",
    "        fig.suptitle(f\"Sample #{index}\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cad6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some of the worst predictions\n",
    "#\n",
    "# The worst ones are expected to be the outliers in the\n",
    "# dataset, as we know we have some bad labeling.\n",
    "for i in range(10):\n",
    "    show_sample(results[i], index=i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb57284",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_size = len(results)\n",
    "# Show the middle ones\n",
    "for i in range(result_size//2-5, result_size//2+5):\n",
    "    show_sample(results[i], index=i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc922a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some of the best predictions\n",
    "for i in range(result_size-10, result_size):\n",
    "    show_sample(results[i], index=i+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
