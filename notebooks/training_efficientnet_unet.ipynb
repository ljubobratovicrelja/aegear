{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  # Adding the core library\n",
    "\n",
    "import maze\n",
    "import importlib\n",
    "importlib.reload(maze)\n",
    "\n",
    "from maze.models import EfficientUNet\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Create a construct which loads the data based in COCO format, with additional heatmaps stored separately, with same naming as the source image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FishHeatmapDataset(Dataset):\n",
    "    def __init__(self, annotation_json, image_dir, heatmap_dir,\n",
    "                 img_transform=None, heatmap_transform=None, joint_transform=None):\n",
    "        self.samples = []\n",
    "        self.img_transform = img_transform\n",
    "        self.heatmap_transform = heatmap_transform\n",
    "        self.joint_transform = joint_transform\n",
    "\n",
    "        with open(annotation_json, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        for img_info in coco_data['images']:\n",
    "            file_name = img_info['file_name']\n",
    "            img_path = os.path.join(image_dir, file_name)\n",
    "            heatmap_path = os.path.join(heatmap_dir, os.path.splitext(file_name)[0] + '.npy')\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            heatmap = np.load(heatmap_path)\n",
    "\n",
    "            self.samples.append((image, heatmap, file_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, heatmap, _ = self.samples[idx]\n",
    "\n",
    "        # Convert to PIL for torchvision transforms\n",
    "        from PIL import Image\n",
    "        image = Image.fromarray(image)\n",
    "        heatmap_img = Image.fromarray((heatmap * 255).astype(np.uint8))\n",
    "\n",
    "        # Apply joint transforms (e.g. crop/flip) identically\n",
    "        if self.joint_transform:\n",
    "            seed = np.random.randint(0, 10000)\n",
    "            torch.manual_seed(seed)\n",
    "            image = self.joint_transform(image)\n",
    "            torch.manual_seed(seed)\n",
    "            heatmap_img = self.joint_transform(heatmap_img)\n",
    "\n",
    "        # Apply individual transforms\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "        if self.heatmap_transform:\n",
    "            heatmap_img = self.heatmap_transform(heatmap_img)\n",
    "\n",
    "        # Normalize heatmap to [0,1] as float32\n",
    "        heatmap_tensor = heatmap_img.float() / 255.0\n",
    "\n",
    "        return image, heatmap_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint transform: same crop/flip on both\n",
    "joint_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "joint_val = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "])\n",
    "\n",
    "# Separate transforms for image and heatmap\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "heatmap_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # just convert to tensor, normalize later\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preload the data\n",
    "\n",
    "We preload the data into DDR memory, as its not so big, and it speeds up the training process massively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Data augmentation for validation\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = FishHeatmapDataset(\n",
    "    annotation_json=\"../data/training/coco/annotations_train.json\",\n",
    "    image_dir='../data/training/coco/images/',\n",
    "    heatmap_dir='../data/training/coco/heatmaps/',\n",
    "    joint_transform=joint_train,\n",
    "    img_transform=img_transform,\n",
    "    heatmap_transform=heatmap_transform,\n",
    ")\n",
    "\n",
    "val_ds = FishHeatmapDataset(\n",
    "    annotation_json=\"../data/training/coco/annotations_val.json\",\n",
    "    image_dir='../data/training/coco/images/',\n",
    "    heatmap_dir='../data/training/coco/heatmaps/',\n",
    "    joint_transform=joint_val,\n",
    "    img_transform=img_transform,\n",
    "    heatmap_transform=heatmap_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig\n",
    "\n",
    "Let's setup our loss, optimizer etc.\n",
    "1. **Loss**: we use Mean Squared Error (MSE) between predicted heatmap and target. It's simple and works well since both are smooth distribution\n",
    "2. **Optimizer**: AdamW \n",
    "3. **LR scheduler**: ReduceLROnPlateau \n",
    "\n",
    "Later we run the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Device selection\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 2. Initialize the model\n",
    "model = EfficientUNet()\n",
    "\n",
    "# Freeze first 3 stages of the encoder (enc1 to enc3)\n",
    "for param in model.enc1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.enc2.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.enc3.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Loss function and optimizer setup.\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training Loop Setup\n",
    "epochs = 10\n",
    "epoch_save_interval = 2\n",
    "\n",
    "model_dir = '../data/models/efficientunet'\n",
    "log_dir = f'{model_dir}/runs'\n",
    "checkpoint_dir = f'{model_dir}/checkpoints'\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir, flush_secs=1)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "    for imgs, heatmaps in train_bar:\n",
    "        imgs = imgs.to(device)\n",
    "        heatmaps = heatmaps.to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = loss_fn(preds, heatmaps)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "    with torch.no_grad():\n",
    "        for imgs, heatmaps in val_bar:\n",
    "            imgs = imgs.to(device)\n",
    "            heatmaps = heatmaps.to(device)\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = loss_fn(preds, heatmaps)\n",
    "            val_loss += loss.item()\n",
    "            val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{model_dir}/best_model.pth')\n",
    "\n",
    "    # Save model checkpoint every N epochs\n",
    "    if (epoch + 1) % epoch_save_interval == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth'))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "Now let's put model through numerous tests to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for inference\n",
    "model.load_state_dict(torch.load(f'{model_dir}/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "total_distances = []\n",
    "within_radius = {r: 0 for r in [3, 5, 10]}\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def get_centroid(heatmap: torch.Tensor):\n",
    "    \"\"\"Returns (x, y) coordinates of the max point in a single heatmap\"\"\"\n",
    "    b, _, h, w = heatmap.shape\n",
    "    flat_idx = torch.argmax(heatmap.view(b, -1), dim=1)\n",
    "    y = flat_idx // w\n",
    "    x = flat_idx % w\n",
    "    return x.cpu().numpy(), y.cpu().numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, heatmaps in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        imgs = imgs.to(device)\n",
    "        heatmaps = heatmaps.to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "\n",
    "        x_pred, y_pred = get_centroid(preds)\n",
    "        x_gt, y_gt = get_centroid(heatmaps)\n",
    "\n",
    "        for xp, yp, xg, yg in zip(x_pred, y_pred, x_gt, y_gt):\n",
    "            dist = np.sqrt((xp - xg)**2 + (yp - yg)**2)\n",
    "            total_distances.append(dist)\n",
    "            for r in within_radius:\n",
    "                if dist <= r:\n",
    "                    within_radius[r] += 1\n",
    "            n_samples += 1\n",
    "\n",
    "        for i in range(imgs.size(0)):\n",
    "            img = imgs[i].cpu()\n",
    "            pred_hm = preds[i, 0].cpu()\n",
    "            gt_hm = heatmaps[i, 0].cpu()\n",
    "\n",
    "            xp, yp = x_pred[i], y_pred[i]\n",
    "            xg, yg = x_gt[i], y_gt[i]\n",
    "            dist = np.sqrt((xp - xg) ** 2 + (yp - yg) ** 2)\n",
    "\n",
    "            results.append({\n",
    "                'image': img,\n",
    "                'gt_heatmap': gt_hm,\n",
    "                'pred_heatmap': pred_hm,\n",
    "                'gt_centroid': (xg, yg),\n",
    "                'pred_centroid': (xp, yp),\n",
    "                'distance': dist,\n",
    "            })\n",
    "\n",
    "avg_dist = np.mean(total_distances)\n",
    "print(f\"\\nAverage centroid distance: {avg_dist:.2f} px\")\n",
    "\n",
    "for r in sorted(within_radius):\n",
    "    print(f\"Within {r}px: {within_radius[r] / n_samples:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort(key=lambda x: -x['distance'])  # descending\n",
    "\n",
    "\n",
    "def show_sample(result, index=None):\n",
    "    img = result['image']\n",
    "    gt = result['gt_heatmap']\n",
    "    pred = result['pred_heatmap']\n",
    "    xg, yg = result['gt_centroid']\n",
    "    xp, yp = result['pred_centroid']\n",
    "    dist = result['distance']\n",
    "\n",
    "    # Denormalize image for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img_disp = img * std + mean\n",
    "    img_disp = img_disp.clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(img_disp)\n",
    "    axs[0].scatter([xg], [yg], c='green', label='GT')\n",
    "    axs[0].scatter([xp], [yp], c='red', label='Pred')\n",
    "    axs[0].set_title(f'Image (Err: {dist:.1f}px)')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].imshow(gt.numpy(), cmap='hot')\n",
    "    axs[1].set_title('GT Heatmap')\n",
    "\n",
    "    axs[2].imshow(pred.numpy(), cmap='hot')\n",
    "    axs[2].set_title('Predicted Heatmap')\n",
    "\n",
    "    if index is not None:\n",
    "        fig.suptitle(f\"Sample #{index}\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show top 5 worst predictions\n",
    "for i in range(5):\n",
    "    show_sample(results[i], index=i+1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
