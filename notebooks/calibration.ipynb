{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"data/videos/2016_0718_200947_002\"  # input video path\n",
    "cornerDataPath = \"data/corners.npy\"\n",
    "calibrationDataPath = \"data/calibration.xml\"\n",
    "readExistingCornerData = False\n",
    "\n",
    "patternSize = (8, 6)  # number of corners in the calibration pattern (rows, columns)\n",
    "subPixWinSize = (5, 5)  # depends on the resolution, 5, 5 seems ok for this camera resolution\n",
    "\n",
    "maxNumSamples = 40  # if there are more than this collected samples, all samples are shuffled and subsampled\n",
    "skipFrames = 120 # how many frames do we skip while seeking video for calibration shots\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sample video for out frame size\n",
    "framePaths = list(map(lambda f: os.path.join(dataPath, f), filter(lambda f: \".png\" in f, os.listdir(dataPath))))\n",
    "\n",
    "frames = list(map(lambda f: cv2.cvtColor(cv2.imread(f), cv2.COLOR_BGR2GRAY), framePaths))\n",
    "\n",
    "print(\"Read {} frames\".format(len(frames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validFrames = []\n",
    "\n",
    "allCorners = []  # collection of all corners for calibration purposes\n",
    "if not readExistingCornerData:\n",
    "\n",
    "    # Enable interactive mode\n",
    "    for frame in tqdm(frames):\n",
    "        quitReading = False\n",
    "\n",
    "        # try detecting the chart\n",
    "        patternWasFound, corners = cv2.findChessboardCorners(frame, patternSize, cv2.CALIB_CB_FAST_CHECK)\n",
    "\n",
    "        if patternWasFound:\n",
    "            corners = cv2.cornerSubPix(frame, corners, subPixWinSize, (-1, -1), criteria)\n",
    "            allCorners.append(corners)\n",
    "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "            cv2.drawChessboardCorners(frame_bgr, patternSize, corners, patternWasFound)\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            validFrames.append(frame)\n",
    "\n",
    "    allCorners = np.array(allCorners)\n",
    "    np.save(cornerDataPath, allCorners)\n",
    "\n",
    "    print(\"Saved corners to {}\".format(cornerDataPath))\n",
    "else:\n",
    "    allCorners = np.load(cornerDataPath)\n",
    "\n",
    "numSamples = allCorners.shape[0]\n",
    "print(\"Collected {} corner sampels.\".format(numSamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up object points\n",
    "patternSquareSideLength = 1.6\n",
    "objPoints = np.zeros((patternSize[0]*patternSize[1], 3), np.float32)\n",
    "objPoints[:, :2] = np.mgrid[0:patternSize[0], 0:patternSize[1]].T.reshape(-1, 2).astype(np.float32)\n",
    "objPoints *= patternSquareSideLength\n",
    "\n",
    "# Assuming `objPoints` is a list of lists or a list of arrays\n",
    "# First, convert it to a numpy array\n",
    "objPoints = np.array([objPoints] * numSamples)\n",
    "\n",
    "objPoints = np.reshape(objPoints, (numSamples, objPoints[0].shape[0], 1, 3))\n",
    "\n",
    "# Then, convert it to a 3-channel float32 array if it's not already\n",
    "if objPoints.dtype != np.float32 or objPoints.shape[-1] != 3:\n",
    "    objPoints = objPoints.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameSize = validFrames[0].shape[::-1]\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objPoints, allCorners, frameSize, None, None)\n",
    "\n",
    "if ret:\n",
    "    print(\"Matrix:\\n{}\".format(mtx))\n",
    "    print(\"Distortion:\\n{}\".format(dist))\n",
    "\n",
    "    storage = cv2.FileStorage(calibrationDataPath, cv2.FILE_STORAGE_WRITE)\n",
    "    storage.write(\"mtx\", mtx)\n",
    "    storage.write(\"dist\", dist)\n",
    "    storage.release()\n",
    "\n",
    "    # print(rvecs)\n",
    "    # print(tvecs)\n",
    "else:\n",
    "    print(\"Calibration failed\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# find optimal camera matrix\n",
    "h, w = frame.shape[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have an image stored in the variable `img`\n",
    "img = validFrames[0]\n",
    "\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# Compute the undistortion and rectification transformation map\n",
    "undistorted_img = cv2.undistort(img, mtx, dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show undistorted image\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(validFrames[0], cmap=\"gray\")\n",
    "ax[1].imshow(undistorted_img, cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
